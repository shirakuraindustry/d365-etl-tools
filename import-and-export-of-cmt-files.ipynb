{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30be53e",
   "metadata": {},
   "source": [
    "Extract the imported zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "35fe4372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files to 'cmt-files/imported-data'\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = os.path.join('cmt-files/imported-data', f'data.zip')\n",
    "extract_dir = 'cmt-files/imported-data'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(zip_path):\n",
    "    print(f\"Error: The file '{zip_path}' does not exist.\")\n",
    "else:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "        print(f\"Extracted files to '{extract_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fce46",
   "metadata": {},
   "source": [
    "To convert data.xml into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15c7da26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to cmt-files/imported-data/msdyn_purchaseorderproduct.parquet\n",
      "Table Name: msdyn_purchaseorderproduct\n",
      "name                             record_id  \\\n",
      "0     00265f8a-7734-f011-8c4c-6045bd51d7ec   \n",
      "1     002951b4-33bd-ef11-b8e8-6045bd511d1a   \n",
      "2     002ba8e3-2b7b-ef11-ac20-000d3acef57b   \n",
      "3     002ec176-738c-f011-b4cb-6045bd5121f4   \n",
      "4     003c2b4d-0587-f011-b4cb-6045bd5121f4   \n",
      "...                                    ...   \n",
      "7366  ffb0f274-fa5a-f011-bec1-6045bd52320c   \n",
      "7367  ffba85ac-1d48-f011-877a-000d3a40b253   \n",
      "7368  ffd368cf-947f-ef11-ac21-002248ef1aca   \n",
      "7369  ffe14cfe-ae91-ef11-8a6a-6045bd5478c8   \n",
      "7370  ffffcb4b-129b-ef11-8a69-6045bd550b8f   \n",
      "\n",
      "name                             createdby createdby_name  \\\n",
      "0     18014a8f-e215-ed11-b83d-000d3acee53f          大江 知広   \n",
      "1     18014a8f-e215-ed11-b83d-000d3acee53f          大江 知広   \n",
      "2     18014a8f-e215-ed11-b83d-000d3acee53f          大江 知広   \n",
      "3     18014a8f-e215-ed11-b83d-000d3acee53f          大江 知広   \n",
      "4     18014a8f-e215-ed11-b83d-000d3acee53f          大江 知広   \n",
      "...                                    ...            ...   \n",
      "7366  18014a8f-e215-ed11-b83d-000d3acee53f          大江 知広   \n",
      "7367  18014a8f-e215-ed11-b83d-000d3acee53f          大江 知広   \n",
      "7368  18014a8f-e215-ed11-b83d-000d3acee53f          大江 知広   \n",
      "7369  18014a8f-e215-ed11-b83d-000d3acee53f          大江 知広   \n",
      "7370  18014a8f-e215-ed11-b83d-000d3acee53f          大江 知広   \n",
      "\n",
      "name                 createdon    exchangerate  \\\n",
      "0    2025-05-19 06:07:32+00:00  1.000000000000   \n",
      "1    2024-12-18 11:32:08+00:00  1.000000000000   \n",
      "2    2024-09-25 10:49:51+00:00  1.000000000000   \n",
      "3    2025-09-08 05:20:06+00:00  1.000000000000   \n",
      "4    2025-09-01 07:28:58+00:00  1.000000000000   \n",
      "...                        ...             ...   \n",
      "7366 2025-07-07 06:20:24+00:00  1.000000000000   \n",
      "7367 2025-06-13 06:14:36+00:00  1.000000000000   \n",
      "7368 2024-10-01 01:31:04+00:00  1.000000000000   \n",
      "7369 2024-10-24 02:23:55+00:00  1.000000000000   \n",
      "7370 2024-11-05 01:07:20+00:00  1.000000000000   \n",
      "\n",
      "name                            modifiedby modifiedby_name  \\\n",
      "0     18014a8f-e215-ed11-b83d-000d3acee53f           大江 知広   \n",
      "1     18014a8f-e215-ed11-b83d-000d3acee53f           大江 知広   \n",
      "2     18014a8f-e215-ed11-b83d-000d3acee53f           大江 知広   \n",
      "3     18014a8f-e215-ed11-b83d-000d3acee53f           大江 知広   \n",
      "4     18014a8f-e215-ed11-b83d-000d3acee53f           大江 知広   \n",
      "...                                    ...             ...   \n",
      "7366  18014a8f-e215-ed11-b83d-000d3acee53f           大江 知広   \n",
      "7367  18014a8f-e215-ed11-b83d-000d3acee53f           大江 知広   \n",
      "7368  18014a8f-e215-ed11-b83d-000d3acee53f           大江 知広   \n",
      "7369  18014a8f-e215-ed11-b83d-000d3acee53f           大江 知広   \n",
      "7370  18014a8f-e215-ed11-b83d-000d3acee53f           大江 知広   \n",
      "\n",
      "name                modifiedon modifiedonbehalfby modifiedonbehalfby_name  \\\n",
      "0    2025-05-23 04:12:10+00:00                NaN                     NaN   \n",
      "1    2025-01-08 10:54:59+00:00                NaN                     NaN   \n",
      "2    2024-09-30 22:35:01+00:00                NaN                     NaN   \n",
      "3    2025-09-10 05:21:38+00:00                NaN                     NaN   \n",
      "4    2025-09-10 06:02:18+00:00                NaN                     NaN   \n",
      "...                        ...                ...                     ...   \n",
      "7366 2025-07-11 04:05:10+00:00                NaN                     NaN   \n",
      "7367 2025-06-17 00:57:22+00:00                NaN                     NaN   \n",
      "7368 2024-11-08 04:50:49+00:00                NaN                     NaN   \n",
      "7369 2024-11-11 08:46:06+00:00                NaN                     NaN   \n",
      "7370 2024-12-04 01:38:43+00:00                NaN                     NaN   \n",
      "\n",
      "name  ... ownerid_name                    owningbusinessunit  \\\n",
      "0     ...        大江 知広  26fa498f-e215-ed11-b83d-000d3acee53f   \n",
      "1     ...        大江 知広  26fa498f-e215-ed11-b83d-000d3acee53f   \n",
      "2     ...        大江 知広  26fa498f-e215-ed11-b83d-000d3acee53f   \n",
      "3     ...        大江 知広  26fa498f-e215-ed11-b83d-000d3acee53f   \n",
      "4     ...        大江 知広  26fa498f-e215-ed11-b83d-000d3acee53f   \n",
      "...   ...          ...                                   ...   \n",
      "7366  ...        大江 知広  26fa498f-e215-ed11-b83d-000d3acee53f   \n",
      "7367  ...        大江 知広  26fa498f-e215-ed11-b83d-000d3acee53f   \n",
      "7368  ...        大江 知広  26fa498f-e215-ed11-b83d-000d3acee53f   \n",
      "7369  ...        大江 知広  26fa498f-e215-ed11-b83d-000d3acee53f   \n",
      "7370  ...        大江 知広  26fa498f-e215-ed11-b83d-000d3acee53f   \n",
      "\n",
      "name owningbusinessunit_name                            owninguser  \\\n",
      "0                orga2351464  18014a8f-e215-ed11-b83d-000d3acee53f   \n",
      "1                orga2351464  18014a8f-e215-ed11-b83d-000d3acee53f   \n",
      "2                orga2351464  18014a8f-e215-ed11-b83d-000d3acee53f   \n",
      "3                orga2351464  18014a8f-e215-ed11-b83d-000d3acee53f   \n",
      "4                orga2351464  18014a8f-e215-ed11-b83d-000d3acee53f   \n",
      "...                      ...                                   ...   \n",
      "7366             orga2351464  18014a8f-e215-ed11-b83d-000d3acee53f   \n",
      "7367             orga2351464  18014a8f-e215-ed11-b83d-000d3acee53f   \n",
      "7368             orga2351464  18014a8f-e215-ed11-b83d-000d3acee53f   \n",
      "7369             orga2351464  18014a8f-e215-ed11-b83d-000d3acee53f   \n",
      "7370             orga2351464  18014a8f-e215-ed11-b83d-000d3acee53f   \n",
      "\n",
      "name owninguser_name statecode statuscode timezoneruleversionnumber  \\\n",
      "0               None         0          1                       NaN   \n",
      "1               None         0          1                       NaN   \n",
      "2               None         0          1                       NaN   \n",
      "3               None         0          1                       NaN   \n",
      "4               None         0          1                       NaN   \n",
      "...              ...       ...        ...                       ...   \n",
      "7366            None         0          1                       NaN   \n",
      "7367            None         0          1                       NaN   \n",
      "7368            None         0          1                       NaN   \n",
      "7369            None         0          1                       NaN   \n",
      "7370            None         0          1                       NaN   \n",
      "\n",
      "name                 transactioncurrencyid transactioncurrencyid_name  \n",
      "0     5888cedb-4416-ed11-b83d-000d3acee53f                          円  \n",
      "1     5888cedb-4416-ed11-b83d-000d3acee53f                          円  \n",
      "2     5888cedb-4416-ed11-b83d-000d3acee53f                          円  \n",
      "3     5888cedb-4416-ed11-b83d-000d3acee53f                          円  \n",
      "4     5888cedb-4416-ed11-b83d-000d3acee53f                          円  \n",
      "...                                    ...                        ...  \n",
      "7366  5888cedb-4416-ed11-b83d-000d3acee53f                          円  \n",
      "7367  5888cedb-4416-ed11-b83d-000d3acee53f                          円  \n",
      "7368  5888cedb-4416-ed11-b83d-000d3acee53f                          円  \n",
      "7369  5888cedb-4416-ed11-b83d-000d3acee53f                          円  \n",
      "7370  5888cedb-4416-ed11-b83d-000d3acee53f                          円  \n",
      "\n",
      "[7371 rows x 45 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the XML file path and schema file path\n",
    "data_file = Path(\"cmt-files/imported-data/data.xml\")\n",
    "schema_file_path = Path(\"cmt-files/imported-data/data_schema.xml\")\n",
    "\n",
    "# Parse the schema file to get field data types\n",
    "def get_field_data_types(schema_path):\n",
    "    tree = ET.parse(schema_path)\n",
    "    root = tree.getroot()\n",
    "    field_data_types = {}\n",
    "\n",
    "    # Extract field names and their types\n",
    "    for field in root.findall(\".//field\"):\n",
    "        name = field.get(\"name\")\n",
    "        field_type = field.get(\"type\")\n",
    "        if name and field_type:\n",
    "            field_data_types[name] = field_type\n",
    "\n",
    "    return field_data_types\n",
    "\n",
    "# Parse the XML file and extract fields with their parent record IDs\n",
    "def parse_xml_with_all_attributes(file_path):\n",
    "    records = []\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract the name attribute from the entity element\n",
    "    table_name = root.find(\".//entity\").get(\"name\") if root.find(\".//entity\") is not None else None\n",
    "\n",
    "    # Iterate over each <record> element\n",
    "    for record in root.findall(\".//record\"):\n",
    "        record_id = record.get(\"id\")  # Extract the record ID\n",
    "        # Iterate over each <field> within the <record>\n",
    "        for field in record.findall(\"field\"):\n",
    "            field_data = {key: field.get(key) for key in field.keys()}  # Extract all field attributes\n",
    "            field_data[\"record_id\"] = record_id  # Add the parent record ID\n",
    "\n",
    "            # Check if lookupentity attribute exists\n",
    "            if field.get(\"lookupentity\"):\n",
    "                # Add lookupentityname as a separate row\n",
    "                records.append({\n",
    "                    \"record_id\": record_id,\n",
    "                    \"name\": field.get(\"name\") + \"_name\",\n",
    "                    \"value\": field.get(\"lookupentityname\")\n",
    "                })\n",
    "\n",
    "            records.append(field_data)\n",
    "\n",
    "    return pd.DataFrame(records), table_name\n",
    "\n",
    "# Parse the XML and create the DataFrame\n",
    "df, table_name = parse_xml_with_all_attributes(data_file)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivoted = df.pivot(index=\"record_id\", columns=\"name\", values=\"value\")\n",
    "\n",
    "# Reset the index if needed\n",
    "df_pivoted.reset_index(inplace=True)\n",
    "\n",
    "# Get field data types from the schema\n",
    "field_data_types = get_field_data_types(schema_file_path)\n",
    "\n",
    "# Apply data types to the pivoted DataFrame\n",
    "for column, dtype in field_data_types.items():\n",
    "    if column in df_pivoted.columns:\n",
    "        try:\n",
    "            if dtype == \"number\" or dtype == \"money\" or dtype == \"float\":\n",
    "                df_pivoted[column] = pd.to_numeric(df_pivoted[column], errors=\"coerce\")\n",
    "            elif dtype == \"datetime\":\n",
    "                df_pivoted[column] = pd.to_datetime(df_pivoted[column], errors=\"coerce\")\n",
    "            elif dtype == \"bool\":\n",
    "                df_pivoted[column] = df_pivoted[column].map({\"true\": True, \"false\": False})\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Could not convert column {column} to {dtype}. Error: {e}\")\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "if table_name:\n",
    "    parquet_file = f\"cmt-files/imported-data/{table_name}.parquet\"\n",
    "    df_pivoted.to_parquet(parquet_file, engine=\"pyarrow\")\n",
    "    print(f\"DataFrame saved to {parquet_file}\")\n",
    "else:\n",
    "    print(\"Table name is not available. DataFrame not saved.\")\n",
    "\n",
    "# Display the pivoted DataFrame and table name\n",
    "print(f\"Table Name: {table_name}\")\n",
    "print(df_pivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e3d91",
   "metadata": {},
   "source": [
    "Create a difference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6c21bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmt-files\\imported-data\\msdyn_purchaseorderproduct.parquet\n",
      "cmt-files\\formatted-data\\msdyn_purchaseorderproduct.parquet\n",
      "New records saved to cmt-files\\difference-data\\msdyn_purchaseorderproduct.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths for the imported and formatted data folders\n",
    "imported_data_path = Path(f\"cmt-files/imported-data/{table_name}.parquet\")\n",
    "formatted_data_path = Path(f\"cmt-files/formatted-data/{table_name}.parquet\")\n",
    "difference_data_path = Path(f\"cmt-files/difference-data/{table_name}.parquet\")\n",
    "print(imported_data_path)\n",
    "print(formatted_data_path)\n",
    "\n",
    "# Load the DataFrames from the Parquet files\n",
    "if imported_data_path.exists() and formatted_data_path.exists():\n",
    "    imported_df = pd.read_parquet(imported_data_path)\n",
    "    formatted_df = pd.read_parquet(formatted_data_path)\n",
    "\n",
    "    # Ensure both DataFrames have a 'modifiedon' column\n",
    "    if 'modifiedon' in imported_df.columns and 'modifiedon' in formatted_df.columns:\n",
    "        # Convert 'modifiedon' columns to datetime for comparison\n",
    "        imported_df['modifiedon'] = pd.to_datetime(imported_df['modifiedon'])\n",
    "        formatted_df['modifiedon'] = pd.to_datetime(formatted_df['modifiedon'])\n",
    "\n",
    "        # Find records in imported_df that are newer than the latest in formatted_df\n",
    "        latest_modifiedon = formatted_df['modifiedon'].max()\n",
    "        new_records_df = imported_df[imported_df['modifiedon'] > latest_modifiedon]\n",
    "\n",
    "        # Save the new records to the difference-data folder\n",
    "        new_records_df.to_parquet(difference_data_path, engine=\"pyarrow\")\n",
    "        print(f\"New records saved to {difference_data_path}\")\n",
    "    else:\n",
    "        print(\"Error: 'modifiedon' column is missing in one of the DataFrames.\")\n",
    "else:\n",
    "    print(\"Error: One or both Parquet files do not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28db473",
   "metadata": {},
   "source": [
    "Merge the Parquet files in the differnce-data folder with those in the formatted-data folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ee57a7d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Merged data saved to cmt-files\\formatted-data\\msdyn_purchaseorderproduct_merged.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Define paths for the schema file and data folders\n",
    "table_name = \"msdyn_purchaseorderproduct\"  # Replace with your actual table name\n",
    "schema_file_path = Path(f\"cmt-files/schema/{table_name}-schema.xml\")\n",
    "difference_data_path = Path(f\"cmt-files/difference-data/{table_name}.parquet\")\n",
    "formatted_data_path = Path(f\"cmt-files/formatted-data/{table_name}.parquet\")\n",
    "merged_data_path = Path(f\"cmt-files/formatted-data/{table_name}_merged.parquet\")\n",
    "\n",
    "# Parse the schema file to get the primaryidfield\n",
    "def get_primaryidfield(schema_path):\n",
    "    tree = ET.parse(schema_path)\n",
    "    root = tree.getroot()\n",
    "    entity = root.find(\".//entity\")\n",
    "    return entity.get(\"primaryidfield\") if entity is not None else None\n",
    "\n",
    "# Get the primaryidfield (guid column)\n",
    "guid_column = get_primaryidfield(schema_file_path)\n",
    "if not guid_column:\n",
    "    print(\"Error: Could not determine the primaryidfield from the schema file.\")\n",
    "else:\n",
    "    # Load the DataFrames from the Parquet files\n",
    "    if difference_data_path.exists() and formatted_data_path.exists():\n",
    "        difference_df = pd.read_parquet(difference_data_path)\n",
    "        formatted_df = pd.read_parquet(formatted_data_path)\n",
    "\n",
    "        # Ensure both DataFrames have the guid and modifiedon columns\n",
    "        if guid_column in difference_df.columns and guid_column in formatted_df.columns and 'modifiedon' in difference_df.columns and 'modifiedon' in formatted_df.columns:\n",
    "            # Convert 'modifiedon' columns to datetime for comparison\n",
    "            difference_df['modifiedon'] = pd.to_datetime(difference_df['modifiedon'])\n",
    "            formatted_df['modifiedon'] = pd.to_datetime(formatted_df['modifiedon'])\n",
    "\n",
    "            # Concatenate the DataFrames\n",
    "            combined_df = pd.concat([formatted_df, difference_df])\n",
    "\n",
    "            # Keep only the latest record for each guid based on modifiedon\n",
    "            merged_df = combined_df.sort_values('modifiedon').drop_duplicates(subset=guid_column, keep='last')\n",
    "\n",
    "            # Save the merged DataFrame to the formatted-data folder\n",
    "            merged_df.to_parquet(merged_data_path, engine=\"pyarrow\")\n",
    "            print(f\"Merged data saved to {merged_data_path}\")\n",
    "        else:\n",
    "            print(f\"Error: '{guid_column}' or 'modifiedon' column is missing in one of the DataFrames.\")\n",
    "    else:\n",
    "        print(\"Error: One or both Parquet files do not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c25d0",
   "metadata": {},
   "source": [
    "Create an export file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "23bc5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported data saved to cmt-files\\data-to-be-exported\\export.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from datetime import datetime\n",
    "\n",
    "table_name = \"msdyn_purchaseorder\"  # Replace with your actual table name\n",
    "\n",
    "# Define paths for the formatted data and export\n",
    "formatted_data_path = Path(f\"cmt-files/formatted-data/{table_name}.parquet\")\n",
    "schema_file_path = Path(f\"cmt-files/schema/{table_name}-schema.xml\")\n",
    "export_data_path = Path(f\"cmt-files/data-to-be-exported/export.parquet\")\n",
    "export_xml_path = Path(f\"cmt-files/data-to-be-exported/data.xml\")\n",
    "schema_export_path = Path(f\"cmt-files/data-to-be-exported/data_schema.xml\")\n",
    "\n",
    "# Load the formatted DataFrame\n",
    "formatted_df = pd.read_parquet(formatted_data_path)\n",
    "\n",
    "# Parse the schema file to get field attributes and entity details\n",
    "def parse_schema(schema_path):\n",
    "    schema_tree = ET.parse(schema_path)\n",
    "    schema_root = schema_tree.getroot()\n",
    "    field_attributes = {}\n",
    "\n",
    "    # Extract field attributes\n",
    "    for field in schema_root.findall(\".//field\"):\n",
    "        name = field.get(\"name\")\n",
    "        if name:\n",
    "            field_attributes[name] = {key: field.get(key) for key in field.keys()}\n",
    "\n",
    "    # Extract entity attributes\n",
    "    entity_element = schema_root.find(\".//entity\")\n",
    "    entity_name = entity_element.get(\"name\") if entity_element is not None else \"unknown_entity\"\n",
    "    display_name = entity_element.get(\"displayname\") if entity_element is not None else \"Unknown Display Name\"\n",
    "\n",
    "    return field_attributes, entity_name, display_name\n",
    "\n",
    "# Precompute lookupentityname_map using schema\n",
    "def precompute_lookupentityname_map(df, field_attributes):\n",
    "    lookupentityname_map = {}\n",
    "    for col in df.columns:\n",
    "        if col.endswith(\"_name\"):\n",
    "            base_name = col[:-5]  # Remove '_name'\n",
    "            if base_name in field_attributes:\n",
    "                lookupentityname_map[base_name] = col\n",
    "    return lookupentityname_map\n",
    "\n",
    "# Load schema field attributes and entity details\n",
    "field_attributes, entity_name, display_name = parse_schema(schema_file_path)\n",
    "lookupentityname_map = precompute_lookupentityname_map(formatted_df, field_attributes)\n",
    "\n",
    "# Save the DataFrame to the exported-data folder as Parquet\n",
    "formatted_df.to_parquet(export_data_path, engine=\"pyarrow\")\n",
    "print(f\"Exported data saved to {export_data_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0277007b",
   "metadata": {},
   "source": [
    "Convert the DataFrame to XML and save it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "a4db04b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported data saved to cmt-files\\data-to-be-exported\\data.xml\n",
      "Schema file saved to cmt-files\\data-to-be-exported\\data_schema.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ohechihiro\\AppData\\Local\\Temp\\ipykernel_13980\\1063858887.py:38: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    }
   ],
   "source": [
    "# Load the exported DataFrame\n",
    "df = pd.read_parquet(Path(export_data_path))\n",
    "\n",
    "# Convert the DataFrame to XML and save it\n",
    "def dataframe_to_custom_xml(df, entity_name, display_name, timestamp, field_attributes, lookupentityname_map):\n",
    "    entities = ET.Element(\"entities\", attrib={\n",
    "        \"xmlns:xsd\": \"http://www.w3.org/2001/XMLSchema\",\n",
    "        \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
    "        \"timestamp\": timestamp\n",
    "    })\n",
    "    entity = ET.SubElement(entities, \"entity\", attrib={\"name\": entity_name, \"displayname\": display_name})\n",
    "    records = ET.SubElement(entity, \"records\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        record_elem = ET.SubElement(records, \"record\")\n",
    "\n",
    "        for col, val in row.items():\n",
    "            if col == \"record_id\":\n",
    "                record_elem.set(\"id\", str(val))\n",
    "            else:\n",
    "                if not col.endswith(\"_name\") and pd.notna(val):\n",
    "                    # Add other fields as child elements\n",
    "                    field_elem = ET.SubElement(record_elem, \"field\", attrib={\"name\": col, \"value\": str(val)})\n",
    "\n",
    "                    # Add attributes from the schema if they exist\n",
    "                    if col in field_attributes:\n",
    "                        for attr_key, attr_val in field_attributes[col].items():\n",
    "                            if attr_key in [\"lookupType\"]:  # Avoid overwriting existing attributes\n",
    "                                field_elem.set(\"lookupentity\", attr_val)\n",
    "\n",
    "                    # If the field name matches a base name, add the lookupentityname attribute\n",
    "                    if col in lookupentityname_map:\n",
    "                        field_elem.set(\"lookupentityname\", str(row[lookupentityname_map[col]]) if pd.notna(row[lookupentityname_map[col]]) else \"\")\n",
    "\n",
    "    return ET.ElementTree(entities)\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "# Convert and save as XML\n",
    "xml_tree = dataframe_to_custom_xml(\n",
    "    df,\n",
    "    entity_name=entity_name,\n",
    "    display_name=display_name,\n",
    "    timestamp=timestamp,\n",
    "    field_attributes=field_attributes,\n",
    "    lookupentityname_map=lookupentityname_map\n",
    ")\n",
    "\n",
    "# Pretty print XML and save it\n",
    "def pretty_print_xml(tree, file_path):\n",
    "    rough_string = ET.tostring(tree.getroot(), encoding=\"utf-8\")\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(reparsed.toprettyxml(indent=\"  \"))\n",
    "\n",
    "pretty_print_xml(xml_tree, export_xml_path)\n",
    "print(f\"Exported data saved to {export_xml_path}\")\n",
    "\n",
    "# Save the schema file to the data-to-be-exported folder\n",
    "schema_tree = ET.parse(schema_file_path)\n",
    "schema_tree.write(schema_export_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "print(f\"Schema file saved to {schema_export_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee0a46b9",
   "metadata": {},
   "source": [
    "Creating data.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1ea7efb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files compressed into cmt-files\\data-to-be-exported\\data.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the paths of the files to be compressed\n",
    "content_types_path = Path(\"cmt-files/data-to-be-exported/[Content_Types].xml\")\n",
    "data_schema_path = Path(\"cmt-files/data-to-be-exported/data_schema.xml\")\n",
    "data_xml_path = Path(\"cmt-files/data-to-be-exported/data.xml\")\n",
    "data_zip_path = Path(\"cmt-files/data-to-be-exported/data.zip\")\n",
    "\n",
    "# Create a zip file and add the files\n",
    "with zipfile.ZipFile(data_zip_path, 'w') as zipf:\n",
    "    zipf.write(content_types_path, arcname=\"[Content_Types].xml\")\n",
    "    zipf.write(data_schema_path, arcname=\"data_schema.xml\")\n",
    "    zipf.write(data_xml_path, arcname=\"data.xml\")\n",
    "\n",
    "print(f\"Files compressed into {data_zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
