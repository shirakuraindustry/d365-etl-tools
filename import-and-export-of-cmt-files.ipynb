{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30be53e",
   "metadata": {},
   "source": [
    "Extract the imported zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "35fe4372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracted files to 'cmt-files/imported-data'\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = os.path.join('cmt-files/imported-data', f'data.zip')\n",
    "extract_dir = 'cmt-files/imported-data'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(zip_path):\n",
    "    print(f\"Error: The file '{zip_path}' does not exist.\")\n",
    "else:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "        print(f\"Extracted files to '{extract_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fce46",
   "metadata": {},
   "source": [
    "To convert data.xml into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15c7da26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame saved to cmt-files/imported-data/account.parquet\n",
      "Table Name: account\n",
      "name                             record_id accountclassificationcode  \\\n",
      "0     00416cb1-4219-ed11-b83e-000d3acf194f                         1   \n",
      "1     00a3c8ae-4219-ed11-b83e-000d3acedc8d                         1   \n",
      "2     019771ab-4219-ed11-b83e-000d3acf194f                         1   \n",
      "3     01a3c8ae-4219-ed11-b83e-000d3acedc8d                         1   \n",
      "4     029771ab-4219-ed11-b83e-000d3acf194f                         1   \n",
      "..                                     ...                       ...   \n",
      "375   f9a1c8ae-4219-ed11-b83e-000d3acedc8d                         1   \n",
      "376   fa7ccba8-4219-ed11-b83e-000d3acedc8d                         1   \n",
      "377   fdb6b908-ded6-ee11-904c-6045bd540c5a                         1   \n",
      "378   fe406cb1-4219-ed11-b83e-000d3acf194f                         1   \n",
      "379   ff406cb1-4219-ed11-b83e-000d3acf194f                         1   \n",
      "\n",
      "name                             accountid accountnumber accountratingcode  \\\n",
      "0     00416cb1-4219-ed11-b83e-000d3acf194f           NaN                 1   \n",
      "1     00a3c8ae-4219-ed11-b83e-000d3acedc8d           NaN                 1   \n",
      "2     019771ab-4219-ed11-b83e-000d3acf194f           NaN                 1   \n",
      "3     01a3c8ae-4219-ed11-b83e-000d3acedc8d           NaN                 1   \n",
      "4     029771ab-4219-ed11-b83e-000d3acf194f           NaN                 1   \n",
      "..                                     ...           ...               ...   \n",
      "375   f9a1c8ae-4219-ed11-b83e-000d3acedc8d           NaN                 1   \n",
      "376   fa7ccba8-4219-ed11-b83e-000d3acedc8d           NaN                 1   \n",
      "377   fdb6b908-ded6-ee11-904c-6045bd540c5a           NaN                 1   \n",
      "378   fe406cb1-4219-ed11-b83e-000d3acf194f           NaN                 1   \n",
      "379   ff406cb1-4219-ed11-b83e-000d3acf194f           NaN                 1   \n",
      "\n",
      "name                    address1_addressid address1_city address1_composite  \\\n",
      "0     3d735588-7974-4f87-9daa-9e28202bee63           NaN                NaN   \n",
      "1     33e3038b-0059-4e47-b759-97db80f6dd08           NaN                NaN   \n",
      "2     ee15d8ae-0466-4c21-9d5b-6119ae2c13e4           NaN                NaN   \n",
      "3     75abd132-10b6-4ec7-921d-765928804ae0           NaN                NaN   \n",
      "4     0dc2fb89-a628-4545-9f9f-442e98ce1b81           NaN                NaN   \n",
      "..                                     ...           ...                ...   \n",
      "375   d1db9b5d-ad5c-408a-a23a-2795e30e8020           NaN                NaN   \n",
      "376   b1dbf6aa-11dd-437b-bd15-0202ea41bf4c           NaN                NaN   \n",
      "377   42713e98-93cf-4e78-9ca2-4f47cae8593a           NaN                NaN   \n",
      "378   4b713900-470d-479f-99ac-7c0406b29c56           NaN                NaN   \n",
      "379   9f22e537-bad2-45e0-b0ea-3d45df15407d           NaN                NaN   \n",
      "\n",
      "name address1_country address1_latitude  ...  sic statecode statuscode  \\\n",
      "0                 NaN               NaN  ...  NaN         1          2   \n",
      "1                 NaN               NaN  ...  NaN         1          2   \n",
      "2                 NaN               NaN  ...  NaN         1          2   \n",
      "3                 NaN               NaN  ...  NaN         1          2   \n",
      "4                 NaN               NaN  ...  NaN         1          2   \n",
      "..                ...               ...  ...  ...       ...        ...   \n",
      "375               NaN               NaN  ...  NaN         1          2   \n",
      "376               NaN               NaN  ...  NaN         1          2   \n",
      "377               NaN               NaN  ...  NaN         1          2   \n",
      "378               NaN               NaN  ...  NaN         1          2   \n",
      "379               NaN               NaN  ...  NaN         1          2   \n",
      "\n",
      "name telephone1 territorycode tickersymbol  \\\n",
      "0           NaN             1          NaN   \n",
      "1           NaN             1          NaN   \n",
      "2           NaN             1          NaN   \n",
      "3           NaN             1          NaN   \n",
      "4           NaN             1          NaN   \n",
      "..          ...           ...          ...   \n",
      "375         NaN             1          NaN   \n",
      "376         NaN             1          NaN   \n",
      "377         NaN             1          NaN   \n",
      "378         NaN             1          NaN   \n",
      "379         NaN             1          NaN   \n",
      "\n",
      "name                 transactioncurrencyid transactioncurrencyid_name  \\\n",
      "0     5888cedb-4416-ed11-b83d-000d3acee53f                          円   \n",
      "1     5888cedb-4416-ed11-b83d-000d3acee53f                          円   \n",
      "2     5888cedb-4416-ed11-b83d-000d3acee53f                          円   \n",
      "3     5888cedb-4416-ed11-b83d-000d3acee53f                          円   \n",
      "4     5888cedb-4416-ed11-b83d-000d3acee53f                          円   \n",
      "..                                     ...                        ...   \n",
      "375   5888cedb-4416-ed11-b83d-000d3acee53f                          円   \n",
      "376   5888cedb-4416-ed11-b83d-000d3acee53f                          円   \n",
      "377   5888cedb-4416-ed11-b83d-000d3acee53f                          円   \n",
      "378   5888cedb-4416-ed11-b83d-000d3acee53f                          円   \n",
      "379   5888cedb-4416-ed11-b83d-000d3acee53f                          円   \n",
      "\n",
      "name websiteurl yominame  \n",
      "0           NaN      NaN  \n",
      "1           NaN      NaN  \n",
      "2           NaN      NaN  \n",
      "3           NaN      NaN  \n",
      "4           NaN      NaN  \n",
      "..          ...      ...  \n",
      "375         NaN      NaN  \n",
      "376         NaN      NaN  \n",
      "377         NaN      NaN  \n",
      "378         NaN      NaN  \n",
      "379         NaN      NaN  \n",
      "\n",
      "[380 rows x 97 columns]\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the XML file path\n",
    "data_file = Path(\"cmt-files/imported-data/data.xml\")\n",
    "\n",
    "# Parse the XML file and extract fields with their parent record IDs\n",
    "def parse_xml_with_all_attributes(file_path):\n",
    "    records = []\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract the name attribute from the entity element\n",
    "    table_name = root.find(\".//entity\").get(\"name\") if root.find(\".//entity\") is not None else None\n",
    "\n",
    "    # Iterate over each <record> element\n",
    "    for record in root.findall(\".//record\"):\n",
    "        record_id = record.get(\"id\")  # Extract the record ID\n",
    "        # Iterate over each <field> within the <record>\n",
    "        for field in record.findall(\"field\"):\n",
    "            field_data = {key: field.get(key) for key in field.keys()}  # Extract all field attributes\n",
    "            field_data[\"record_id\"] = record_id  # Add the parent record ID\n",
    "\n",
    "            # Check if lookupentity attribute exists\n",
    "            if field.get(\"lookupentity\"):\n",
    "                # Add lookupentityname as a separate row\n",
    "                records.append({\n",
    "                    \"record_id\": record_id,\n",
    "                    \"name\": field.get(\"name\") + \"_name\",\n",
    "                    \"value\": field.get(\"lookupentityname\")\n",
    "                })\n",
    "\n",
    "            records.append(field_data)\n",
    "\n",
    "    return pd.DataFrame(records), table_name\n",
    "\n",
    "# Parse the XML and create the DataFrame\n",
    "df, table_name = parse_xml_with_all_attributes(data_file)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivoted = df.pivot(index=\"record_id\", columns=\"name\", values=\"value\")\n",
    "\n",
    "# Reset the index if needed\n",
    "df_pivoted.reset_index(inplace=True)\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "if table_name:\n",
    "    parquet_file = f\"cmt-files/imported-data/{table_name}.parquet\"\n",
    "    df_pivoted.to_parquet(parquet_file, engine=\"pyarrow\")\n",
    "    print(f\"DataFrame saved to {parquet_file}\")\n",
    "else:\n",
    "    print(\"Table name is not available. DataFrame not saved.\")\n",
    "\n",
    "# Display the pivoted DataFrame and table name\n",
    "print(f\"Table Name: {table_name}\")\n",
    "print(df_pivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e3d91",
   "metadata": {},
   "source": [
    "Create a difference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6c21bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmt-files\\imported-data\\account.parquet\n",
      "cmt-files\\formatted-data\\account.parquet\n",
      "New records saved to cmt-files\\difference-data\\account.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths for the imported and formatted data folders\n",
    "imported_data_path = Path(f\"cmt-files/imported-data/{table_name}.parquet\")\n",
    "formatted_data_path = Path(f\"cmt-files/formatted-data/{table_name}.parquet\")\n",
    "difference_data_path = Path(f\"cmt-files/difference-data/{table_name}.parquet\")\n",
    "print(imported_data_path)\n",
    "print(formatted_data_path)\n",
    "\n",
    "# Load the DataFrames from the Parquet files\n",
    "if imported_data_path.exists() and formatted_data_path.exists():\n",
    "    imported_df = pd.read_parquet(imported_data_path)\n",
    "    formatted_df = pd.read_parquet(formatted_data_path)\n",
    "\n",
    "    # Ensure both DataFrames have a 'modifiedon' column\n",
    "    if 'modifiedon' in imported_df.columns and 'modifiedon' in formatted_df.columns:\n",
    "        # Convert 'modifiedon' columns to datetime for comparison\n",
    "        imported_df['modifiedon'] = pd.to_datetime(imported_df['modifiedon'])\n",
    "        formatted_df['modifiedon'] = pd.to_datetime(formatted_df['modifiedon'])\n",
    "\n",
    "        # Find records in imported_df that are newer than the latest in formatted_df\n",
    "        latest_modifiedon = formatted_df['modifiedon'].max()\n",
    "        new_records_df = imported_df[imported_df['modifiedon'] > latest_modifiedon]\n",
    "\n",
    "        # Save the new records to the difference-data folder\n",
    "        new_records_df.to_parquet(difference_data_path, engine=\"pyarrow\")\n",
    "        print(f\"New records saved to {difference_data_path}\")\n",
    "    else:\n",
    "        print(\"Error: 'modifiedon' column is missing in one of the DataFrames.\")\n",
    "else:\n",
    "    print(\"Error: One or both Parquet files do not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c25d0",
   "metadata": {},
   "source": [
    "Create an export file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "23bc5361",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported data saved to cmt-files\\data-to-be-exported\\export.parquet\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ohechihiro\\AppData\\Local\\Temp\\ipykernel_31076\\847069684.py:90: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  timestamp = datetime.utcnow().isoformat() + \"Z\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exported data saved to cmt-files\\data-to-be-exported\\data.xml\n",
      "Schema file saved to cmt-files\\data-to-be-exported\\data_schema.xml\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from datetime import datetime\n",
    "\n",
    "table_name = \"account\"  # Replace with your actual table name\n",
    "\n",
    "# Define paths for the formatted data and export\n",
    "formatted_data_path = Path(f\"cmt-files/formatted-data/{table_name}.parquet\")\n",
    "schema_file_path = Path(f\"cmt-files/schema/{table_name}-schema.xml\")\n",
    "export_data_path = Path(f\"cmt-files/data-to-be-exported/export.parquet\")\n",
    "export_xml_path = Path(f\"cmt-files/data-to-be-exported/data.xml\")\n",
    "schema_export_path = Path(f\"cmt-files/data-to-be-exported/data_schema.xml\")\n",
    "\n",
    "# Load the formatted DataFrame\n",
    "formatted_df = pd.read_parquet(formatted_data_path)\n",
    "\n",
    "# Parse the schema file to get field attributes and entity details\n",
    "def parse_schema(schema_path):\n",
    "    schema_tree = ET.parse(schema_path)\n",
    "    schema_root = schema_tree.getroot()\n",
    "    field_attributes = {}\n",
    "\n",
    "    # Extract field attributes\n",
    "    for field in schema_root.findall(\".//field\"):\n",
    "        name = field.get(\"name\")\n",
    "        if name:\n",
    "            field_attributes[name] = {key: field.get(key) for key in field.keys()}\n",
    "\n",
    "    # Extract entity attributes\n",
    "    entity_element = schema_root.find(\".//entity\")\n",
    "    entity_name = entity_element.get(\"name\") if entity_element is not None else \"unknown_entity\"\n",
    "    display_name = entity_element.get(\"displayname\") if entity_element is not None else \"Unknown Display Name\"\n",
    "\n",
    "    return field_attributes, entity_name, display_name\n",
    "\n",
    "# Precompute lookupentityname_map using schema\n",
    "def precompute_lookupentityname_map(df, field_attributes):\n",
    "    lookupentityname_map = {}\n",
    "    for col in df.columns:\n",
    "        if col.endswith(\"_name\"):\n",
    "            base_name = col[:-5]  # Remove '_name'\n",
    "            if base_name in field_attributes:\n",
    "                lookupentityname_map[base_name] = col\n",
    "    return lookupentityname_map\n",
    "\n",
    "# Load schema field attributes and entity details\n",
    "field_attributes, entity_name, display_name = parse_schema(schema_file_path)\n",
    "lookupentityname_map = precompute_lookupentityname_map(formatted_df, field_attributes)\n",
    "\n",
    "# Save the DataFrame to the exported-data folder as Parquet\n",
    "formatted_df.to_parquet(export_data_path, engine=\"pyarrow\")\n",
    "print(f\"Exported data saved to {export_data_path}\")\n",
    "\n",
    "# Convert the DataFrame to XML and save it\n",
    "def dataframe_to_custom_xml(df, entity_name, display_name, timestamp, field_attributes, lookupentityname_map):\n",
    "    entities = ET.Element(\"entities\", attrib={\n",
    "        \"xmlns:xsd\": \"http://www.w3.org/2001/XMLSchema\",\n",
    "        \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
    "        \"timestamp\": timestamp\n",
    "    })\n",
    "    entity = ET.SubElement(entities, \"entity\", attrib={\"name\": entity_name, \"displayname\": display_name})\n",
    "    records = ET.SubElement(entity, \"records\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        record_elem = ET.SubElement(records, \"record\")\n",
    "\n",
    "        for col, val in row.items():\n",
    "            if col == \"record_id\":\n",
    "                record_elem.set(\"id\", str(val))\n",
    "            else:\n",
    "                if not col.endswith(\"_name\") and pd.notna(val):\n",
    "                    # Add other fields as child elements\n",
    "                    field_elem = ET.SubElement(record_elem, \"field\", attrib={\"name\": col, \"value\": str(val)})\n",
    "\n",
    "                    # Add attributes from the schema if they exist\n",
    "                    if col in field_attributes:\n",
    "                        for attr_key, attr_val in field_attributes[col].items():\n",
    "                            if attr_key in [\"lookupType\"]:  # Avoid overwriting existing attributes\n",
    "                                field_elem.set(\"lookupentity\", attr_val)\n",
    "\n",
    "                    # If the field name matches a base name, add the lookupentityname attribute\n",
    "                    if col in lookupentityname_map:\n",
    "                        field_elem.set(\"lookupentityname\", str(row[lookupentityname_map[col]]) if pd.notna(row[lookupentityname_map[col]]) else \"\")\n",
    "\n",
    "    return ET.ElementTree(entities)\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "# Convert and save as XML\n",
    "xml_tree = dataframe_to_custom_xml(\n",
    "    formatted_df,\n",
    "    entity_name=entity_name,\n",
    "    display_name=display_name,\n",
    "    timestamp=timestamp,\n",
    "    field_attributes=field_attributes,\n",
    "    lookupentityname_map=lookupentityname_map\n",
    ")\n",
    "\n",
    "# Pretty print XML and save it\n",
    "def pretty_print_xml(tree, file_path):\n",
    "    rough_string = ET.tostring(tree.getroot(), encoding=\"utf-8\")\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(reparsed.toprettyxml(indent=\"  \"))\n",
    "\n",
    "pretty_print_xml(xml_tree, export_xml_path)\n",
    "print(f\"Exported data saved to {export_xml_path}\")\n",
    "\n",
    "# Save the schema file to the data-to-be-exported folder\n",
    "schema_tree = ET.parse(schema_file_path)\n",
    "schema_tree.write(schema_export_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "print(f\"Schema file saved to {schema_export_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1ea7efb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files compressed into cmt-files\\data-to-be-exported\\data.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the paths of the files to be compressed\n",
    "content_types_path = Path(\"cmt-files/data-to-be-exported/[Content_Types].xml\")\n",
    "data_schema_path = Path(\"cmt-files/data-to-be-exported/data_schema.xml\")\n",
    "data_xml_path = Path(\"cmt-files/data-to-be-exported/data.xml\")\n",
    "data_zip_path = Path(\"cmt-files/data-to-be-exported/data.zip\")\n",
    "\n",
    "# Create a zip file and add the files\n",
    "with zipfile.ZipFile(data_zip_path, 'w') as zipf:\n",
    "    zipf.write(content_types_path, arcname=\"[Content_Types].xml\")\n",
    "    zipf.write(data_schema_path, arcname=\"data_schema.xml\")\n",
    "    zipf.write(data_xml_path, arcname=\"data.xml\")\n",
    "\n",
    "print(f\"Files compressed into {data_zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
