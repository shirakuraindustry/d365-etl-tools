{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e30be53e",
   "metadata": {},
   "source": [
    "Extract the imported zip file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "35fe4372",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error: The file 'cmt-files/imported-data\\data-data.zip' does not exist.\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "\n",
    "zip_path = os.path.join('cmt-files/imported-data', f'data-data.zip')\n",
    "extract_dir = 'cmt-files/imported-data'\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(zip_path):\n",
    "    print(f\"Error: The file '{zip_path}' does not exist.\")\n",
    "else:\n",
    "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "        zip_ref.extractall(extract_dir)\n",
    "        print(f\"Extracted files to '{extract_dir}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69fce46",
   "metadata": {},
   "source": [
    "To convert data.xml into a Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c7da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the XML file path\n",
    "data_file = Path(\"cmt-files/imported-data/data.xml\")\n",
    "\n",
    "# Parse the XML file and extract fields with their parent record IDs\n",
    "def parse_xml_with_all_attributes(file_path):\n",
    "    records = []\n",
    "    tree = ET.parse(file_path)\n",
    "    root = tree.getroot()\n",
    "\n",
    "    # Extract the name attribute from the entity element\n",
    "    table_name = root.find(\".//entity\").get(\"name\") if root.find(\".//entity\") is not None else None\n",
    "\n",
    "    # Iterate over each <record> element\n",
    "    for record in root.findall(\".//record\"):\n",
    "        record_id = record.get(\"id\")  # Extract the record ID\n",
    "        # Iterate over each <field> within the <record>\n",
    "        for field in record.findall(\"field\"):\n",
    "            field_data = {key: field.get(key) for key in field.keys()}  # Extract all field attributes\n",
    "            field_data[\"record_id\"] = record_id  # Add the parent record ID\n",
    "\n",
    "            # Check if lookupentity attribute exists\n",
    "            if field.get(\"lookupentity\"):\n",
    "                # Add lookupentityname as a separate row\n",
    "                records.append({\n",
    "                    \"record_id\": record_id,\n",
    "                    \"name\": field.get(\"name\") + \"_name\",\n",
    "                    \"value\": field.get(\"lookupentityname\")\n",
    "                })\n",
    "\n",
    "            records.append(field_data)\n",
    "\n",
    "    return pd.DataFrame(records), table_name\n",
    "\n",
    "# Parse the XML and create the DataFrame\n",
    "df, table_name = parse_xml_with_all_attributes(data_file)\n",
    "\n",
    "# Pivot the DataFrame\n",
    "df_pivoted = df.pivot(index=\"record_id\", columns=\"name\", values=\"value\")\n",
    "\n",
    "# Reset the index if needed\n",
    "df_pivoted.reset_index(inplace=True)\n",
    "\n",
    "# Save the DataFrame to a Parquet file\n",
    "if table_name:\n",
    "    parquet_file = f\"cmt-files/imported-data/{table_name}.parquet\"\n",
    "    df_pivoted.to_parquet(parquet_file, engine=\"pyarrow\")\n",
    "    print(f\"DataFrame saved to {parquet_file}\")\n",
    "else:\n",
    "    print(\"Table name is not available. DataFrame not saved.\")\n",
    "\n",
    "# Display the pivoted DataFrame and table name\n",
    "print(f\"Table Name: {table_name}\")\n",
    "print(df_pivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e3d91",
   "metadata": {},
   "source": [
    "Create a difference file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6c21bdb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cmt-files\\imported-data\\account.parquet\n",
      "cmt-files\\formatted-data\\account.parquet\n",
      "New records saved to cmt-files\\difference-data\\account.parquet\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# Define paths for the imported and formatted data folders\n",
    "imported_data_path = Path(f\"cmt-files/imported-data/{table_name}.parquet\")\n",
    "formatted_data_path = Path(f\"cmt-files/formatted-data/{table_name}.parquet\")\n",
    "difference_data_path = Path(f\"cmt-files/difference-data/{table_name}.parquet\")\n",
    "print(imported_data_path)\n",
    "print(formatted_data_path)\n",
    "\n",
    "# Load the DataFrames from the Parquet files\n",
    "if imported_data_path.exists() and formatted_data_path.exists():\n",
    "    imported_df = pd.read_parquet(imported_data_path)\n",
    "    formatted_df = pd.read_parquet(formatted_data_path)\n",
    "\n",
    "    # Ensure both DataFrames have a 'modifiedon' column\n",
    "    if 'modifiedon' in imported_df.columns and 'modifiedon' in formatted_df.columns:\n",
    "        # Convert 'modifiedon' columns to datetime for comparison\n",
    "        imported_df['modifiedon'] = pd.to_datetime(imported_df['modifiedon'])\n",
    "        formatted_df['modifiedon'] = pd.to_datetime(formatted_df['modifiedon'])\n",
    "\n",
    "        # Find records in imported_df that are newer than the latest in formatted_df\n",
    "        latest_modifiedon = formatted_df['modifiedon'].max()\n",
    "        new_records_df = imported_df[imported_df['modifiedon'] > latest_modifiedon]\n",
    "\n",
    "        # Save the new records to the difference-data folder\n",
    "        new_records_df.to_parquet(difference_data_path, engine=\"pyarrow\")\n",
    "        print(f\"New records saved to {difference_data_path}\")\n",
    "    else:\n",
    "        print(\"Error: 'modifiedon' column is missing in one of the DataFrames.\")\n",
    "else:\n",
    "    print(\"Error: One or both Parquet files do not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "472c25d0",
   "metadata": {},
   "source": [
    "Create an export file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23bc5361",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import xml.etree.ElementTree as ET\n",
    "from xml.dom import minidom\n",
    "from datetime import datetime\n",
    "\n",
    "# Define paths for the formatted data and export\n",
    "formatted_data_path = Path(f\"cmt-files/formatted-data/{table_name}.parquet\")\n",
    "schema_file_path = Path(f\"cmt-files/schema/{table_name}-schema.xml\")\n",
    "export_data_path = Path(f\"cmt-files/data-to-be-exported/export.parquet\")\n",
    "export_xml_path = Path(f\"cmt-files/data-to-be-exported/data.xml\")\n",
    "schema_export_path = Path(f\"cmt-files/data-to-be-exported/data_schema.xml\")\n",
    "\n",
    "# Load the formatted DataFrame\n",
    "formatted_df = pd.read_parquet(formatted_data_path)\n",
    "\n",
    "# Parse the schema file to get field attributes and entity details\n",
    "def parse_schema(schema_path):\n",
    "    schema_tree = ET.parse(schema_path)\n",
    "    schema_root = schema_tree.getroot()\n",
    "    field_attributes = {}\n",
    "\n",
    "    # Extract field attributes\n",
    "    for field in schema_root.findall(\".//field\"):\n",
    "        name = field.get(\"name\")\n",
    "        if name:\n",
    "            field_attributes[name] = {key: field.get(key) for key in field.keys()}\n",
    "\n",
    "    # Extract entity attributes\n",
    "    entity_element = schema_root.find(\".//entity\")\n",
    "    entity_name = entity_element.get(\"name\") if entity_element is not None else \"unknown_entity\"\n",
    "    display_name = entity_element.get(\"displayname\") if entity_element is not None else \"Unknown Display Name\"\n",
    "\n",
    "    return field_attributes, entity_name, display_name\n",
    "\n",
    "# Precompute lookupentityname_map using schema\n",
    "def precompute_lookupentityname_map(df, field_attributes):\n",
    "    lookupentityname_map = {}\n",
    "    for col in df.columns:\n",
    "        if col.endswith(\"_name\"):\n",
    "            base_name = col[:-5]  # Remove '_name'\n",
    "            if base_name in field_attributes:\n",
    "                lookupentityname_map[base_name] = col\n",
    "    return lookupentityname_map\n",
    "\n",
    "# Load schema field attributes and entity details\n",
    "field_attributes, entity_name, display_name = parse_schema(schema_file_path)\n",
    "lookupentityname_map = precompute_lookupentityname_map(formatted_df, field_attributes)\n",
    "\n",
    "# Save the DataFrame to the exported-data folder as Parquet\n",
    "formatted_df.to_parquet(export_data_path, engine=\"pyarrow\")\n",
    "print(f\"Exported data saved to {export_data_path}\")\n",
    "\n",
    "# Convert the DataFrame to XML and save it\n",
    "def dataframe_to_custom_xml(df, entity_name, display_name, timestamp, field_attributes, lookupentityname_map):\n",
    "    entities = ET.Element(\"entities\", attrib={\n",
    "        \"xmlns:xsd\": \"http://www.w3.org/2001/XMLSchema\",\n",
    "        \"xmlns:xsi\": \"http://www.w3.org/2001/XMLSchema-instance\",\n",
    "        \"timestamp\": timestamp\n",
    "    })\n",
    "    entity = ET.SubElement(entities, \"entity\", attrib={\"name\": entity_name, \"displayname\": display_name})\n",
    "    records = ET.SubElement(entity, \"records\")\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        record_elem = ET.SubElement(records, \"record\")\n",
    "\n",
    "        for col, val in row.items():\n",
    "            if not col.endswith(\"_name\") and pd.notna(val):\n",
    "                # Add other fields as child elements\n",
    "                field_elem = ET.SubElement(record_elem, \"field\", attrib={\"name\": col, \"value\": str(val)})\n",
    "\n",
    "                # Add attributes from the schema if they exist\n",
    "                if col in field_attributes:\n",
    "                    for attr_key, attr_val in field_attributes[col].items():\n",
    "                        if attr_key not in [\"name\", \"value\"]:  # Avoid overwriting existing attributes\n",
    "                            field_elem.set(attr_key, attr_val)\n",
    "\n",
    "                # If the field name matches a base name, add the lookupentityname attribute\n",
    "                if col in lookupentityname_map:\n",
    "                    field_elem.set(\"lookupentityname\", str(row[lookupentityname_map[col]]) if pd.notna(row[lookupentityname_map[col]]) else \"\")\n",
    "\n",
    "    return ET.ElementTree(entities)\n",
    "\n",
    "# Get the current timestamp\n",
    "timestamp = datetime.utcnow().isoformat() + \"Z\"\n",
    "\n",
    "# Convert and save as XML\n",
    "xml_tree = dataframe_to_custom_xml(\n",
    "    formatted_df,\n",
    "    entity_name=entity_name,\n",
    "    display_name=display_name,\n",
    "    timestamp=timestamp,\n",
    "    field_attributes=field_attributes,\n",
    "    lookupentityname_map=lookupentityname_map\n",
    ")\n",
    "\n",
    "# Pretty print XML and save it\n",
    "def pretty_print_xml(tree, file_path):\n",
    "    rough_string = ET.tostring(tree.getroot(), encoding=\"utf-8\")\n",
    "    reparsed = minidom.parseString(rough_string)\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(reparsed.toprettyxml(indent=\"  \"))\n",
    "\n",
    "pretty_print_xml(xml_tree, export_xml_path)\n",
    "print(f\"Exported data saved to {export_xml_path}\")\n",
    "\n",
    "# Save the schema file to the data-to-be-exported folder\n",
    "schema_tree = ET.parse(schema_file_path)\n",
    "schema_tree.write(schema_export_path, encoding=\"utf-8\", xml_declaration=True)\n",
    "print(f\"Schema file saved to {schema_export_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "559b9f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files copied to cmt-files\\data-to-be-exported\\data\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the paths of the files and the target folder\n",
    "content_types_path = Path(\"cmt-files/data-to-be-exported/[Content_Types].xml\")\n",
    "data_schema_path = Path(\"cmt-files/data-to-be-exported/data_schema.xml\")\n",
    "data_xml_path = Path(\"cmt-files/data-to-be-exported/data.xml\")\n",
    "data_folder_path = Path(\"cmt-files/data-to-be-exported/data\")\n",
    "\n",
    "# Create the target folder if it doesn't exist\n",
    "data_folder_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Copy the files into the target folder\n",
    "shutil.copy(content_types_path, data_folder_path / \"[Content_Types].xml\")\n",
    "shutil.copy(data_schema_path, data_folder_path / \"data_schema.xml\")\n",
    "shutil.copy(data_xml_path, data_folder_path / \"data.xml\")\n",
    "\n",
    "print(f\"Files copied to {data_folder_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e4ad33a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Folder cmt-files\\data-to-be-exported\\data compressed into cmt-files\\data-to-be-exported\\data.zip\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path of the folder to be compressed and the output zip file\n",
    "data_folder_path = Path(\"cmt-files/data-to-be-exported/data\")\n",
    "data_zip_path = Path(\"cmt-files/data-to-be-exported/data.zip\")\n",
    "\n",
    "# Create a zip file and add the contents of the data folder\n",
    "with zipfile.ZipFile(data_zip_path, 'w') as zipf:\n",
    "    for file in data_folder_path.rglob(\"*\"):\n",
    "        zipf.write(file, arcname=file.relative_to(data_folder_path))\n",
    "\n",
    "print(f\"Folder {data_folder_path} compressed into {data_zip_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017ae1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "# Define the path of the folder to be compressed and the output zip file\n",
    "data_folder_path = Path(\"cmt-files/data-to-be-exported/data\")\n",
    "data_zip_path = Path(\"cmt-files/data-to-be-exported/data.zip\")\n",
    "\n",
    "# Ensure the folder exists\n",
    "if not data_folder_path.exists() or not data_folder_path.is_dir():\n",
    "    print(f\"Error: Folder {data_folder_path} does not exist or is not a directory.\")\n",
    "else:\n",
    "    # Create a zip file and add the contents of the data folder\n",
    "    with zipfile.ZipFile(data_zip_path, 'w') as zipf:\n",
    "        for root, _, files in os.walk(data_folder_path):\n",
    "            for file in files:\n",
    "                file_path = Path(root) / file\n",
    "                zipf.write(file_path, arcname=file_path.relative_to(data_folder_path))\n",
    "\n",
    "    print(f\"Folder {data_folder_path} compressed into {data_zip_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
